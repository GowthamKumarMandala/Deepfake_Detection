{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8af45e3",
   "metadata": {},
   "source": [
    "\n",
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4996e830",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'splitfolders'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msplitfolders\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptimizers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Adam\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'splitfolders'"
     ]
    }
   ],
   "source": [
    "import dlib\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from pylab import *\n",
    "from PIL import Image, ImageChops, ImageEnhance\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import splitfolders\n",
    "import PIL\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import Model\n",
    "import glob\n",
    "import cv2\n",
    "import seaborn as sns\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dense, Dropout, BatchNormalization, LSTM\n",
    "import tensorflow.keras.backend as K\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import InceptionV3, ResNet50\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Concatenate, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import matplotlib.pyplot as plt\n",
    "import os, shutil\n",
    "import sys\n",
    "import shutil\n",
    "import json\n",
    "import glob as gb\n",
    "import dlib\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeffcf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "videos_folder = \"train_sample_videos\"\n",
    "\n",
    "\n",
    "metadata_file = os.path.join(videos_folder, \"metadata.json\")\n",
    "\n",
    "with open(metadata_file, 'r') as json_file:\n",
    "    metadata = json.load(json_file)\n",
    "\n",
    "\n",
    "for video_filename, video_info in metadata.items():\n",
    "    label = video_info[\"label\"]\n",
    "    source_path = os.path.join(videos_folder, video_filename)\n",
    "    destination_folder = os.path.join(videos_folder, label.upper())\n",
    "\n",
    "\n",
    "    if not os.path.exists(destination_folder):\n",
    "        os.makedirs(destination_folder)\n",
    "\n",
    "    \n",
    "    destination_path = os.path.join(destination_folder, video_filename)\n",
    "    shutil.move(source_path, destination_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b79a30",
   "metadata": {},
   "source": [
    "# Preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828163ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "real_videos_folder = 'train_sample_videos/REAL'\n",
    "fake_videos_folder = 'train_sample_videos/FAKE'\n",
    "\n",
    "def process_videos(video_folder, output_folder):\n",
    "    list_of_data = [f for f in os.listdir(video_folder) if f.endswith('.mp4')]\n",
    "    detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "    for vid in list_of_data:\n",
    "        count = 0\n",
    "        cap = cv2.VideoCapture(os.path.join(video_folder, vid))\n",
    "\n",
    "        while cap.isOpened():\n",
    "            ret, frame = cap.read()\n",
    "\n",
    "            if not ret or frame is None or frame.size == 0:\n",
    "                break\n",
    "\n",
    "            face_rects, _, _ = detector.run(frame, 0)\n",
    "\n",
    "            for i, d in enumerate(face_rects):\n",
    "                x1 = d.left()\n",
    "                y1 = d.top()\n",
    "                x2 = d.right()\n",
    "                y2 = d.bottom()\n",
    "                crop_img = frame[y1:y2, x1:x2]\n",
    "\n",
    "                if crop_img.size != 0:\n",
    "                    resized_img = cv2.resize(crop_img, (128, 128))\n",
    "                    cv2.imwrite(os.path.join(output_folder, vid.split('.')[0] + '_' + str(count) + '.png'), resized_img)\n",
    "                    count += 1\n",
    "\n",
    "    cap.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb59207",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_videos(real_videos_folder, 'data/real')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d580d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_videos(fake_videos_folder, 'data/fake')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88f5c1c9",
   "metadata": {},
   "source": [
    "# Loading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc281d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class config:\n",
    "    \n",
    "    data_path = 'data/'\n",
    "    \n",
    "    path_train = \"./output/train\"\n",
    "    path_test = \"./output/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5109eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitfolders.ratio(config.data_path, output=\"output\", seed=101, ratio=(.8, .2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ae7fac8",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf1772",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "IMAGE_SHAPE = (128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8be56fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = \"output/train\"\n",
    "VAL_PATH = \"output/val\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78749fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1/255)\n",
    "\n",
    "train_gen = datagen.flow_from_directory(directory = TRAIN_PATH, \n",
    "                                          class_mode=\"categorical\",\n",
    "                                          target_size = IMAGE_SHAPE,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          color_mode='rgb',\n",
    "                                          seed = 1234,\n",
    "                                          shuffle = True)\n",
    "\n",
    "val_gen = datagen.flow_from_directory(directory = VAL_PATH, \n",
    "                                          class_mode=\"categorical\",\n",
    "                                          target_size = IMAGE_SHAPE,\n",
    "                                          batch_size = BATCH_SIZE,\n",
    "                                          color_mode='rgb',\n",
    "                                          seed = 1234,\n",
    "                                          shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "424e0cae",
   "metadata": {},
   "source": [
    "# Data Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f948013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_images(generator, num_images=9):\n",
    "    \n",
    "    images, labels = generator.next()\n",
    "\n",
    "    \n",
    "    plt.figure(figsize=(10, 10))\n",
    "    for i in range(num_images):\n",
    "        plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(images[i])\n",
    "        plt.title(f'Class: {labels[i]}')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plot_images(train_gen)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09bfdef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_images(val_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ccb15d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_counts = np.sum(train_gen.labels, axis=0)\n",
    "classes = list(train_gen.class_indices.keys())\n",
    "\n",
    "plt.bar(classes, class_counts)\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Class Distribution in Training Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d103fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pixel_intensity(image):\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "   \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(image[:, :, 0].ravel(), bins=256, color='red', alpha=0.5, rwidth=0.8)\n",
    "    plt.title('Red Channel')\n",
    "    plt.xlabel('Pixel Intensity')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.hist(image[:, :, 1].ravel(), bins=256, color='green', alpha=0.5, rwidth=0.8)\n",
    "    plt.title('Green Channel')\n",
    "    plt.xlabel('Pixel Intensity')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.hist(image[:, :, 2].ravel(), bins=256, color='blue', alpha=0.5, rwidth=0.8)\n",
    "    plt.title('Blue Channel')\n",
    "    plt.xlabel('Pixel Intensity')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "sample_image = train_gen[0][0][0]  \n",
    "plot_pixel_intensity(sample_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27534ab8",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430bca74",
   "metadata": {},
   "source": [
    "### Inception-ResNet-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4995f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "def specificity_m(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
    "    specificity = true_negatives / (possible_negatives + K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "def sensitivity_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    sensitivity = true_positives / (possible_positives + K.epsilon())\n",
    "    return sensitivity\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true - y_pred))\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607e67ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False, weights='imagenet', input_shape=(128, 128, 3), pooling='max')\n",
    "\n",
    "\n",
    "x31 = Flatten()(inc.output)\n",
    "predictionss = Dense(2, activation='softmax')(x31)\n",
    "\n",
    "\n",
    "model = Model(inputs = inc.inputs, outputs = predictionss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c20d0254",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1_score, recall_m, precision_m, specificity_m, sensitivity_m, mae, mse])\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=50, steps_per_epoch=len(train_gen), validation_steps=len(val_gen), callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d27245",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/Inceptionresnet_v2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2d18893",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = history.history['accuracy']\n",
    "train_recall = history.history['recall_m']\n",
    "train_precision = history.history['precision_m']\n",
    "train_f1 = history.history['f1_score']\n",
    "train_sensitivity = history.history['sensitivity_m']\n",
    "train_specificity = history.history['specificity_m']\n",
    "\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_recall = history.history['val_recall_m']\n",
    "val_precision = history.history['val_precision_m']\n",
    "val_f1 = history.history['val_f1_score']\n",
    "val_sensitivity = history.history['val_sensitivity_m']\n",
    "val_specificity = history.history['val_specificity_m']\n",
    "\n",
    "train_mae = history.history['mae'] \n",
    "train_mse = history.history['mse']  \n",
    "\n",
    "val_mae = history.history['val_mae']  \n",
    "val_mse = history.history['val_mse']\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(12, 16))  # 4 rows, 2 columns\n",
    "\n",
    "# Plot accuracy\n",
    "axs[0, 0].plot(train_acc, label='Train')\n",
    "axs[0, 0].plot(val_acc, label='Validation')\n",
    "axs[0, 0].set_title('Accuracy')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Plot precision\n",
    "axs[0, 1].plot(train_precision, label='Train')\n",
    "axs[0, 1].plot(val_precision, label='Validation')\n",
    "axs[0, 1].set_title('Precision')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Plot recall\n",
    "axs[1, 0].plot(train_recall, label='Train')\n",
    "axs[1, 0].plot(val_recall, label='Validation')\n",
    "axs[1, 0].set_title('Recall')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Plot F1 score\n",
    "axs[1, 1].plot(train_f1, label='Train')\n",
    "axs[1, 1].plot(val_f1, label='Validation')\n",
    "axs[1, 1].set_title('F1 Score')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# Plot sensitivity\n",
    "axs[2, 0].plot(train_sensitivity, label='Train')\n",
    "axs[2, 0].plot(val_sensitivity, label='Validation')\n",
    "axs[2, 0].set_title('Sensitivity')\n",
    "axs[2, 0].legend()\n",
    "\n",
    "# Plot specificity\n",
    "axs[2, 1].plot(train_specificity, label='Train')\n",
    "axs[2, 1].plot(val_specificity, label='Validation')\n",
    "axs[2, 1].set_title('Specificity')\n",
    "axs[2, 1].legend()\n",
    "\n",
    "# Plot MAE\n",
    "axs[3, 0].plot(train_mae, label='Train')\n",
    "axs[3, 0].plot(val_mae, label='Validation')\n",
    "axs[3, 0].set_title('MAE')\n",
    "axs[3, 0].legend()\n",
    "\n",
    "# Plot MSE\n",
    "axs[3, 1].plot(train_mse, label='Train')\n",
    "axs[3, 1].plot(val_mse, label='Validation')\n",
    "axs[3, 1].set_title('MSE')\n",
    "axs[3, 1].legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3798925c",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_evaluation = model.evaluate(val_gen, steps=len(val_gen))\n",
    "val_accuracy = val_evaluation[1]\n",
    "\n",
    "# Predictions on the validation data\n",
    "val_predictions = model.predict(val_gen)\n",
    "val_pred_classes = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# True classes from the validation data generator\n",
    "val_true_classes = val_gen.classes\n",
    "class_names = list(val_gen.class_indices.keys())\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion = confusion_matrix(val_true_classes, val_pred_classes)\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f376bc57",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = history.history['accuracy'][-1]\n",
    "f = history.history['f1_score'][-1]\n",
    "p = history.history['precision_m'][-1]\n",
    "r = history.history['recall_m'][-1]\n",
    "mae = history.history['mae'][-1]\n",
    "mse = history.history['mse'][-1]\n",
    "specificity = history.history['specificity_m'][-1]  \n",
    "sensitivity = history.history['sensitivity_m'][-1]\n",
    "print('Accuracy = ' + str(a * 100))\n",
    "print('Precision = ' + str(p * 100))\n",
    "print('F1 Score = ' + str(f * 100))\n",
    "print('Recall = ' + str(r * 100))\n",
    "print('MAE = ' + str(mae))\n",
    "print('MSE = ' + str(mse))\n",
    "print('Sensitivity = ' + str(sensitivity * 100))  \n",
    "print('Specificity = ' + str(specificity * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "944b74f8",
   "metadata": {},
   "source": [
    "# VGG19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7794b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "def specificity_m(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
    "    specificity = true_negatives / (possible_negatives + K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "def sensitivity_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    sensitivity = true_positives / (possible_positives + K.epsilon())\n",
    "    return sensitivity\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true - y_pred))\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980a72bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = tf.keras.applications.vgg19.VGG19(include_top=False, weights='imagenet', input_shape=(128, 128, 3), pooling='max')\n",
    "\n",
    "\n",
    "x31 = Flatten()(inc.output)\n",
    "predictionss = Dense(2, activation='softmax')(x31)\n",
    "\n",
    "\n",
    "model = Model(inputs = inc.inputs, outputs = predictionss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8d85f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1_score, recall_m, precision_m, specificity_m, sensitivity_m, mae, mse])\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=50, steps_per_epoch=len(train_gen), validation_steps=len(val_gen), callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779476b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/vgg19.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48c50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = history.history['accuracy']\n",
    "train_recall = history.history['recall_m']\n",
    "train_precision = history.history['precision_m']\n",
    "train_f1 = history.history['f1_score']\n",
    "train_sensitivity = history.history['sensitivity_m']\n",
    "train_specificity = history.history['specificity_m']\n",
    "\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_recall = history.history['val_recall_m']\n",
    "val_precision = history.history['val_precision_m']\n",
    "val_f1 = history.history['val_f1_score']\n",
    "val_sensitivity = history.history['val_sensitivity_m']\n",
    "val_specificity = history.history['val_specificity_m']\n",
    "\n",
    "train_mae = history.history['mae'] \n",
    "train_mse = history.history['mse']  \n",
    "\n",
    "val_mae = history.history['val_mae']  \n",
    "val_mse = history.history['val_mse']\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(12, 16))  # 4 rows, 2 columns\n",
    "\n",
    "# Plot accuracy\n",
    "axs[0, 0].plot(train_acc, label='Train')\n",
    "axs[0, 0].plot(val_acc, label='Validation')\n",
    "axs[0, 0].set_title('Accuracy')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Plot precision\n",
    "axs[0, 1].plot(train_precision, label='Train')\n",
    "axs[0, 1].plot(val_precision, label='Validation')\n",
    "axs[0, 1].set_title('Precision')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Plot recall\n",
    "axs[1, 0].plot(train_recall, label='Train')\n",
    "axs[1, 0].plot(val_recall, label='Validation')\n",
    "axs[1, 0].set_title('Recall')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Plot F1 score\n",
    "axs[1, 1].plot(train_f1, label='Train')\n",
    "axs[1, 1].plot(val_f1, label='Validation')\n",
    "axs[1, 1].set_title('F1 Score')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# Plot sensitivity\n",
    "axs[2, 0].plot(train_sensitivity, label='Train')\n",
    "axs[2, 0].plot(val_sensitivity, label='Validation')\n",
    "axs[2, 0].set_title('Sensitivity')\n",
    "axs[2, 0].legend()\n",
    "\n",
    "# Plot specificity\n",
    "axs[2, 1].plot(train_specificity, label='Train')\n",
    "axs[2, 1].plot(val_specificity, label='Validation')\n",
    "axs[2, 1].set_title('Specificity')\n",
    "axs[2, 1].legend()\n",
    "\n",
    "# Plot MAE\n",
    "axs[3, 0].plot(train_mae, label='Train')\n",
    "axs[3, 0].plot(val_mae, label='Validation')\n",
    "axs[3, 0].set_title('MAE')\n",
    "axs[3, 0].legend()\n",
    "\n",
    "# Plot MSE\n",
    "axs[3, 1].plot(train_mse, label='Train')\n",
    "axs[3, 1].plot(val_mse, label='Validation')\n",
    "axs[3, 1].set_title('MSE')\n",
    "axs[3, 1].legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea267ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_evaluation = model.evaluate(val_gen, steps=len(val_gen))\n",
    "val_accuracy = val_evaluation[1]\n",
    "\n",
    "# Predictions on the validation data\n",
    "val_predictions = model.predict(val_gen)\n",
    "val_pred_classes = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# True classes from the validation data generator\n",
    "val_true_classes = val_gen.classes\n",
    "class_names = list(val_gen.class_indices.keys())\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion = confusion_matrix(val_true_classes, val_pred_classes)\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6968570b",
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = history.history['accuracy'][-1]\n",
    "f1 = history.history['f1_score'][-1]\n",
    "p1 = history.history['precision_m'][-1]\n",
    "r1 = history.history['recall_m'][-1]\n",
    "mae1 = history.history['mae'][-1]\n",
    "mse1 = history.history['mse'][-1]\n",
    "specificity1 = history.history['specificity_m'][-1]  \n",
    "sensitivity1 = history.history['sensitivity_m'][-1]\n",
    "print('Accuracy = ' + str(a1 * 100))\n",
    "print('Precision = ' + str(p1 * 100))\n",
    "print('F1 Score = ' + str(f1 * 100))\n",
    "print('Recall = ' + str(r1 * 100))\n",
    "print('MAE = ' + str(mae1))\n",
    "print('MSE = ' + str(mse1))\n",
    "print('Sensitivity = ' + str(sensitivity1 * 100))  \n",
    "print('Specificity = ' + str(specificity1 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4a281e",
   "metadata": {},
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "480a5776",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "def specificity_m(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
    "    specificity = true_negatives / (possible_negatives + K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "def sensitivity_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    sensitivity = true_positives / (possible_positives + K.epsilon())\n",
    "    return sensitivity\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true - y_pred))\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ba528f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.Sequential() \n",
    "\n",
    "\n",
    "model.add(Conv2D(filters=32, kernel_size=3, strides=1, padding='same', activation='relu', input_shape=[128, 128, 3]))\n",
    "model.add(MaxPooling2D(2, ))\n",
    "model.add(Conv2D(filters=64, kernel_size=3, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling2D(2))\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7875815",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1_score, recall_m, precision_m, specificity_m, sensitivity_m, mae, mse])\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=50, steps_per_epoch=len(train_gen), validation_steps=len(val_gen), callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34243cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/cnn.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1603c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = history.history['accuracy']\n",
    "train_recall = history.history['recall_m']\n",
    "train_precision = history.history['precision_m']\n",
    "train_f1 = history.history['f1_score']\n",
    "train_sensitivity = history.history['sensitivity_m']\n",
    "train_specificity = history.history['specificity_m']\n",
    "\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_recall = history.history['val_recall_m']\n",
    "val_precision = history.history['val_precision_m']\n",
    "val_f1 = history.history['val_f1_score']\n",
    "val_sensitivity = history.history['val_sensitivity_m']\n",
    "val_specificity = history.history['val_specificity_m']\n",
    "\n",
    "train_mae = history.history['mae'] \n",
    "train_mse = history.history['mse']  \n",
    "\n",
    "val_mae = history.history['val_mae']  \n",
    "val_mse = history.history['val_mse']\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(12, 16))  # 4 rows, 2 columns\n",
    "\n",
    "# Plot accuracy\n",
    "axs[0, 0].plot(train_acc, label='Train')\n",
    "axs[0, 0].plot(val_acc, label='Validation')\n",
    "axs[0, 0].set_title('Accuracy')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Plot precision\n",
    "axs[0, 1].plot(train_precision, label='Train')\n",
    "axs[0, 1].plot(val_precision, label='Validation')\n",
    "axs[0, 1].set_title('Precision')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Plot recall\n",
    "axs[1, 0].plot(train_recall, label='Train')\n",
    "axs[1, 0].plot(val_recall, label='Validation')\n",
    "axs[1, 0].set_title('Recall')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Plot F1 score\n",
    "axs[1, 1].plot(train_f1, label='Train')\n",
    "axs[1, 1].plot(val_f1, label='Validation')\n",
    "axs[1, 1].set_title('F1 Score')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# Plot sensitivity\n",
    "axs[2, 0].plot(train_sensitivity, label='Train')\n",
    "axs[2, 0].plot(val_sensitivity, label='Validation')\n",
    "axs[2, 0].set_title('Sensitivity')\n",
    "axs[2, 0].legend()\n",
    "\n",
    "# Plot specificity\n",
    "axs[2, 1].plot(train_specificity, label='Train')\n",
    "axs[2, 1].plot(val_specificity, label='Validation')\n",
    "axs[2, 1].set_title('Specificity')\n",
    "axs[2, 1].legend()\n",
    "\n",
    "# Plot MAE\n",
    "axs[3, 0].plot(train_mae, label='Train')\n",
    "axs[3, 0].plot(val_mae, label='Validation')\n",
    "axs[3, 0].set_title('MAE')\n",
    "axs[3, 0].legend()\n",
    "\n",
    "# Plot MSE\n",
    "axs[3, 1].plot(train_mse, label='Train')\n",
    "axs[3, 1].plot(val_mse, label='Validation')\n",
    "axs[3, 1].set_title('MSE')\n",
    "axs[3, 1].legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b984c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation data\n",
    "val_evaluation = model.evaluate(val_gen, steps=len(val_gen))\n",
    "val_accuracy = val_evaluation[1]\n",
    "\n",
    "# Predictions on the validation data\n",
    "val_predictions = model.predict(val_gen)\n",
    "val_pred_classes = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# True classes from the validation data generator\n",
    "val_true_classes = val_gen.classes\n",
    "\n",
    "# Retrieve class labels (class names) from the generator\n",
    "class_names = list(val_gen.class_indices.keys())\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion = confusion_matrix(val_true_classes, val_pred_classes)\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11301f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2 = history.history['accuracy'][-1]\n",
    "f2 = history.history['f1_score'][-1]\n",
    "p2 = history.history['precision_m'][-1]\n",
    "r2 = history.history['recall_m'][-1]\n",
    "mae2 = history.history['mae'][-1]\n",
    "mse2 = history.history['mse'][-1]\n",
    "specificity2 = history.history['specificity_m'][-1]  \n",
    "sensitivity2 = history.history['sensitivity_m'][-1]\n",
    "print('Accuracy = ' + str(a2 * 100))\n",
    "print('Precision = ' + str(p2 * 100))\n",
    "print('F1 Score = ' + str(f2 * 100))\n",
    "print('Recall = ' + str(r2 * 100))\n",
    "print('MAE = ' + str(mae2))\n",
    "print('MSE = ' + str(mse2))\n",
    "print('Sensitivity = ' + str(sensitivity2 * 100))  \n",
    "print('Specificity = ' + str(specificity2 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8224c4a5",
   "metadata": {},
   "source": [
    "# Xception"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85859fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    \n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "def specificity_m(y_true, y_pred):\n",
    "    true_negatives = K.sum(K.round(K.clip((1 - y_true) * (1 - y_pred), 0, 1)))\n",
    "    possible_negatives = K.sum(K.round(K.clip(1 - y_true, 0, 1)))\n",
    "    specificity = true_negatives / (possible_negatives + K.epsilon())\n",
    "    return specificity\n",
    "\n",
    "def sensitivity_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    sensitivity = true_positives / (possible_positives + K.epsilon())\n",
    "    return sensitivity\n",
    "\n",
    "def mae(y_true, y_pred):\n",
    "    return K.mean(K.abs(y_true - y_pred))\n",
    "\n",
    "def mse(y_true, y_pred):\n",
    "    return K.mean(K.square(y_true - y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29ccc3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inc = tf.keras.applications.xception.Xception(include_top=False, weights='imagenet', input_shape=(128, 128, 3), pooling='max')\n",
    "\n",
    "\n",
    "x31 = Flatten()(inc.output)\n",
    "predictionss = Dense(2, activation='softmax')(x31)\n",
    "\n",
    "\n",
    "model = Model(inputs = inc.inputs, outputs = predictionss)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49cb9d96",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', f1_score, recall_m, precision_m, specificity_m, sensitivity_m, mae, mse])\n",
    "history = model.fit(train_gen, validation_data=val_gen, epochs=50, steps_per_epoch=len(train_gen), validation_steps=len(val_gen), callbacks=[early_stopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eab4ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('models/Xception.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eef963d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = history.history['accuracy']\n",
    "train_recall = history.history['recall_m']\n",
    "train_precision = history.history['precision_m']\n",
    "train_f1 = history.history['f1_score']\n",
    "train_sensitivity = history.history['sensitivity_m']\n",
    "train_specificity = history.history['specificity_m']\n",
    "\n",
    "val_acc = history.history['val_accuracy']\n",
    "val_recall = history.history['val_recall_m']\n",
    "val_precision = history.history['val_precision_m']\n",
    "val_f1 = history.history['val_f1_score']\n",
    "val_sensitivity = history.history['val_sensitivity_m']\n",
    "val_specificity = history.history['val_specificity_m']\n",
    "\n",
    "train_mae = history.history['mae'] \n",
    "train_mse = history.history['mse']  \n",
    "\n",
    "val_mae = history.history['val_mae']  \n",
    "val_mse = history.history['val_mse']\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(4, 2, figsize=(12, 16))  # 4 rows, 2 columns\n",
    "\n",
    "# Plot accuracy\n",
    "axs[0, 0].plot(train_acc, label='Train')\n",
    "axs[0, 0].plot(val_acc, label='Validation')\n",
    "axs[0, 0].set_title('Accuracy')\n",
    "axs[0, 0].legend()\n",
    "\n",
    "# Plot precision\n",
    "axs[0, 1].plot(train_precision, label='Train')\n",
    "axs[0, 1].plot(val_precision, label='Validation')\n",
    "axs[0, 1].set_title('Precision')\n",
    "axs[0, 1].legend()\n",
    "\n",
    "# Plot recall\n",
    "axs[1, 0].plot(train_recall, label='Train')\n",
    "axs[1, 0].plot(val_recall, label='Validation')\n",
    "axs[1, 0].set_title('Recall')\n",
    "axs[1, 0].legend()\n",
    "\n",
    "# Plot F1 score\n",
    "axs[1, 1].plot(train_f1, label='Train')\n",
    "axs[1, 1].plot(val_f1, label='Validation')\n",
    "axs[1, 1].set_title('F1 Score')\n",
    "axs[1, 1].legend()\n",
    "\n",
    "# Plot sensitivity\n",
    "axs[2, 0].plot(train_sensitivity, label='Train')\n",
    "axs[2, 0].plot(val_sensitivity, label='Validation')\n",
    "axs[2, 0].set_title('Sensitivity')\n",
    "axs[2, 0].legend()\n",
    "\n",
    "# Plot specificity\n",
    "axs[2, 1].plot(train_specificity, label='Train')\n",
    "axs[2, 1].plot(val_specificity, label='Validation')\n",
    "axs[2, 1].set_title('Specificity')\n",
    "axs[2, 1].legend()\n",
    "\n",
    "# Plot MAE\n",
    "axs[3, 0].plot(train_mae, label='Train')\n",
    "axs[3, 0].plot(val_mae, label='Validation')\n",
    "axs[3, 0].set_title('MAE')\n",
    "axs[3, 0].legend()\n",
    "\n",
    "# Plot MSE\n",
    "axs[3, 1].plot(train_mse, label='Train')\n",
    "axs[3, 1].plot(val_mse, label='Validation')\n",
    "axs[3, 1].set_title('MSE')\n",
    "axs[3, 1].legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c327ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the validation data\n",
    "val_evaluation = model.evaluate(val_gen, steps=len(val_gen))\n",
    "val_accuracy = val_evaluation[1]\n",
    "\n",
    "# Predictions on the validation data\n",
    "val_predictions = model.predict(val_gen)\n",
    "val_pred_classes = np.argmax(val_predictions, axis=1)\n",
    "\n",
    "# True classes from the validation data generator\n",
    "val_true_classes = val_gen.classes\n",
    "\n",
    "# Retrieve class labels (class names) from the generator\n",
    "class_names = list(val_gen.class_indices.keys())\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "confusion = confusion_matrix(val_true_classes, val_pred_classes)\n",
    "\n",
    "# Create a heatmap of the confusion matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.set(font_scale=1.2)\n",
    "sns.heatmap(confusion, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False,\n",
    "            xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a469848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a3 = history.history['accuracy'][-1]\n",
    "f3 = history.history['f1_score'][-1]\n",
    "p3 = history.history['precision_m'][-1]\n",
    "r3 = history.history['recall_m'][-1]\n",
    "mae3 = history.history['mae'][-1]\n",
    "mse3 = history.history['mse'][-1]\n",
    "specificity3 = history.history['specificity_m'][-1]  \n",
    "sensitivity3 = history.history['sensitivity_m'][-1]\n",
    "print('Accuracy = ' + str(a3 * 100))\n",
    "print('Precision = ' + str(p3 * 100))\n",
    "print('F1 Score = ' + str(f3 * 100))\n",
    "print('Recall = ' + str(r3 * 100))\n",
    "print('MAE = ' + str(mae3))\n",
    "print('MSE = ' + str(mse3))\n",
    "print('Sensitivity = ' + str(sensitivity3 * 100))  \n",
    "print('Specificity = ' + str(specificity3 * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324e1a57",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37047c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results ={'Accuracy': [a,a1,a2,a3],\n",
    " 'Recall':[r,r1,r2,r3],\n",
    " 'Precision': [p,p1,p2,p3],\n",
    "  'F1 Score' : [f,f1,f2,f3],\n",
    "    'Sensitivity' : [sensitivity,sensitivity1,sensitivity2,sensitivity3],\n",
    "    'Specificity' : [specificity,specificity1,specificity2,specificity3],\n",
    "   'MAE' : [mae,mae1,mae2,mae3],\n",
    "    'MSE' : [mse,mse1,mse2,mse3]}\n",
    "index = ['InceptionResnet V2','VGG19','CNN','Xception']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc63c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "results =pd.DataFrame(results,index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c9117b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig =results.plot(kind='scatter',title='Comaprison of models',figsize =(19,19)).get_figure()\n",
    "fig.savefig('Final Result.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aead4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results, index=['InceptionResnet V2', 'VGG19', 'CNN', 'Xception'])\n",
    "\n",
    "# Plotting Accuracy\n",
    "plt.figure(figsize=(10, 6))\n",
    "results_df['Accuracy'].plot(kind='scatter', color='skyblue')\n",
    "plt.title('Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbca30d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "results_df['Recall'].plot(kind='scatter', color='salmon')\n",
    "plt.title('Recall Comparison')\n",
    "plt.ylabel('Recall')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc43813",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "results_df['Precision'].plot(kind='scatter', color='lightgreen')\n",
    "plt.title('Precision Comparison')\n",
    "plt.ylabel('Precision')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58d16b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "results_df['F1 Score'].plot(kind='scatter', color='orange')\n",
    "plt.title('F1 Score Comparison')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8741ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "results_df['Sensitivity'].plot(kind='scatter', color='purple')\n",
    "plt.title('Sensitivity Comparison')\n",
    "plt.ylabel('Sensitivity')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20407ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "results_df['Specificity'].plot(kind='scatter', color='pink')\n",
    "plt.title('Specificity Comparison')\n",
    "plt.ylabel('Specificity')\n",
    "plt.ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec2a955",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
